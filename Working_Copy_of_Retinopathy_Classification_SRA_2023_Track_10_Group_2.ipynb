{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nB-aa1orEaW-",
        "outputId": "947c15f2-ae95-4015-a2db-b1e916a807b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UT_s0FZcimEU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.nn import Conv2d\n",
        "from torchvision.transforms import Compose\n",
        "from torchvision.transforms import Resize\n",
        "from torchvision.transforms import CenterCrop\n",
        "from PIL import Image\n",
        "from torchvision.transforms.functional import pil_to_tensor\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Cx5Dh9O1fB7D"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from torch.utils.data import random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpmfQuwrk5iH",
        "outputId": "cdbe6e4d-56e5-42a6-e5de-9f4733cd5998"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# SAANVI COMPUTER CODE\n",
        "data = '/content/drive/MyDrive/TrainingData_Final'\n",
        "print()\n",
        "dirs = os.listdir(data)\n",
        "dataset_old = []\n",
        "for dir in dirs:\n",
        "  for inner_dir in os.listdir(os.path.join(data, dir)):\n",
        "      dataset_old.append(os.path.join(data, dir, inner_dir))\n",
        "# print(dataset_old)\n",
        "dataset = []\n",
        "curr = 0\n",
        "for i in dataset_old:\n",
        "  if i[42] != '5':\n",
        "    dataset.append(i)\n",
        "  elif curr < 400:\n",
        "    dataset.append(i)\n",
        "    curr += 1\n",
        "#42nd index\n",
        "# print(len(dataset), curr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WBnU5deX5W-T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad5b9731-38a7-4dee-b5d4-30ca289ae6ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 train:  439\n",
            "0 test:  55\n",
            "0 val:  55\n",
            "---\n",
            "1 train:  796\n",
            "1 test:  100\n",
            "1 val:  100\n",
            "---\n",
            "2 train:  128\n",
            "2 test:  16\n",
            "2 val:  17\n",
            "---\n",
            "3 train:  153\n",
            "3 test:  19\n",
            "3 val:  20\n",
            "---\n",
            "4 train:  28\n",
            "4 test:  3\n",
            "4 val:  4\n",
            "---\n",
            "5 train:  320\n",
            "5 test:  40\n",
            "5 val:  40\n",
            "---\n",
            "total: 2333\n",
            "train: 1864\n",
            "test: 233\n",
            "val: 236\n"
          ]
        }
      ],
      "source": [
        "# SAANVI COMPUTER CODE\n",
        "train_set = []\n",
        "test_set = []\n",
        "validation_set = []\n",
        "for i in range(0, 6):\n",
        "  path = f\"/content/drive/MyDrive/TrainingData_Final/{i}\"\n",
        "  all_imgs_0 = os.listdir(path)\n",
        "  # /content/drive/MyDrive/TrainingData_Final/0\n",
        "  all_imgs_0 = [f\"{path}/{_path}\" for _path in all_imgs_0]\n",
        "  # print(all_imgs_0)\n",
        "  if i == 5:\n",
        "    all_imgs_0 = all_imgs_0[:400]\n",
        "    # print(len(all_imgs_0))\n",
        "  num_imgs_0 = len(all_imgs_0)\n",
        "  arr = np.arange(num_imgs_0)\n",
        "  np.random.shuffle(arr)\n",
        "  # print(i, 'total:', num_imgs_0)\n",
        "  curr_tr = len(train_set)\n",
        "  curr_te = len(test_set)\n",
        "  curr_va = len(validation_set)\n",
        "  train_set += [all_imgs_0[idx] for idx in arr[0:int(0.8*num_imgs_0)]]\n",
        "  test_set += [all_imgs_0[idx]  for idx in arr[int(0.8*num_imgs_0):int(0.9*num_imgs_0)]]\n",
        "  validation_set += [all_imgs_0[idx]  for idx in arr[int(0.9*num_imgs_0):int(num_imgs_0)]]\n",
        "  print(i, 'train: ', len(train_set) - curr_tr)\n",
        "  print(i, 'test: ', len(test_set) - curr_te)\n",
        "  print(i, 'val: ', len(validation_set) - curr_va)\n",
        "  print('---')\n",
        "print('total:', len(train_set) + len(test_set) + len(validation_set))\n",
        "print('train:', len(train_set))\n",
        "print('test:', len(test_set))\n",
        "print('val:', len(validation_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wDWjyKB755Gs"
      },
      "outputs": [],
      "source": [
        "# SAANVI COMPUTER CODE\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "labels = []\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, file_paths):\n",
        "        self.file_paths = file_paths\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((256, 256)),  # Resize the image to (256, 256)\n",
        "            transforms.ToTensor(),          # Convert the PIL image to a PyTorch tensor\n",
        "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize the tensor\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = self.file_paths[idx]\n",
        "        image = Image.open(file_path)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "\n",
        "        return image, file_path[42]\n",
        "train_dataloader = DataLoader(CustomDataset(train_set), batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(CustomDataset(test_set), batch_size=64, shuffle=True)\n",
        "val_dataloader = DataLoader(CustomDataset(validation_set), batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SzkBSXIZ1eRF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8929686c-7374-4aec-dba1-926aad40ec06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Using {} device'.format(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xvZliuxgXN_8"
      },
      "outputs": [],
      "source": [
        "train_features, train_labels = next(iter(train_dataloader))\n",
        "img = train_features[0].squeeze()\n",
        "label = train_labels[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8CpZzVrY1j1Q"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NeuralNetwork, self).__init__()\n",
        "    self.linear_relu_stack = nn.Sequential(\n",
        "      nn.Conv2d(3, 4, kernel_size=3),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2, stride = 2),\n",
        "      nn.Conv2d(4, 8, kernel_size=3),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2, stride = 2),\n",
        "      nn.Conv2d(8, 16, kernel_size=3),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2, stride = 2),\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(14400, 5000),\n",
        "      nn.Linear(5000, 5), #nn.Linear(10000, 6)\n",
        "      nn.Softmax(dim=1)\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    logits = self.linear_relu_stack(x)\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NeuralNetwork2, self).__init__()\n",
        "    self.linear_relu_stack = nn.Sequential(\n",
        "      nn.Conv2d(3, 4, kernel_size=3),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2, stride = 2),\n",
        "      nn.Conv2d(4, 8, kernel_size=3),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2, stride = 2),\n",
        "      nn.Conv2d(8, 16, kernel_size=3),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2, stride = 2),\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(14400, 5000),\n",
        "      nn.Linear(5000, 2), #nn.Linear(10000, 6)\n",
        "      # nn.Sigmoid(),\n",
        "      # nn.softmax\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    logits = self.linear_relu_stack(x)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "u8CzVAQNoU-R"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "2t45q8xKqc22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dfd31455-8801-486f-a831-b1827a5aaa00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (4): ReLU()\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (7): ReLU()\n",
            "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (9): Flatten(start_dim=1, end_dim=-1)\n",
            "    (10): Linear(in_features=14400, out_features=5000, bias=True)\n",
            "    (11): Linear(in_features=5000, out_features=5, bias=True)\n",
            "    (12): Softmax(dim=1)\n",
            "  )\n",
            ")\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 1.610418 [    0/ 1864]\n",
            "4.0\n",
            "loss: 1.389208 [   64/ 1864]\n",
            "33.0\n",
            "loss: 1.529833 [  128/ 1864]\n",
            "24.0\n",
            "loss: 1.592333 [  192/ 1864]\n",
            "20.0\n",
            "loss: 1.451708 [  256/ 1864]\n",
            "29.0\n",
            "loss: 1.467333 [  320/ 1864]\n",
            "28.0\n",
            "loss: 1.576708 [  384/ 1864]\n",
            "21.0\n",
            "loss: 1.436083 [  448/ 1864]\n",
            "30.0\n",
            "loss: 1.436083 [  512/ 1864]\n",
            "30.0\n",
            "loss: 1.467333 [  576/ 1864]\n",
            "28.0\n",
            "loss: 1.436083 [  640/ 1864]\n",
            "30.0\n",
            "loss: 1.545458 [  704/ 1864]\n",
            "23.0\n",
            "loss: 1.342333 [  768/ 1864]\n",
            "36.0\n",
            "loss: 1.342333 [  832/ 1864]\n",
            "36.0\n",
            "loss: 1.467333 [  896/ 1864]\n",
            "28.0\n",
            "loss: 1.561083 [  960/ 1864]\n",
            "22.0\n",
            "loss: 1.436083 [ 1024/ 1864]\n",
            "30.0\n",
            "loss: 1.514208 [ 1088/ 1864]\n",
            "25.0\n",
            "loss: 1.467333 [ 1152/ 1864]\n",
            "28.0\n",
            "loss: 1.514208 [ 1216/ 1864]\n",
            "25.0\n",
            "loss: 1.467333 [ 1280/ 1864]\n",
            "28.0\n",
            "loss: 1.436083 [ 1344/ 1864]\n",
            "30.0\n",
            "loss: 1.545458 [ 1408/ 1864]\n",
            "23.0\n",
            "loss: 1.498583 [ 1472/ 1864]\n",
            "26.0\n",
            "loss: 1.482958 [ 1536/ 1864]\n",
            "27.0\n",
            "loss: 1.498583 [ 1600/ 1864]\n",
            "26.0\n",
            "loss: 1.467333 [ 1664/ 1864]\n",
            "28.0\n",
            "loss: 1.561083 [ 1728/ 1864]\n",
            "22.0\n",
            "loss: 1.451708 [ 1792/ 1864]\n",
            "29.0\n",
            "loss: 1.279832 [  232/ 1864]\n",
            "5.0\n",
            "Test Error: \n",
            " Accuracy: 42.9%, Avg loss: 0.025544 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.529833 [    0/ 1864]\n",
            "24.0\n",
            "loss: 1.576708 [   64/ 1864]\n",
            "21.0\n",
            "loss: 1.451708 [  128/ 1864]\n",
            "29.0\n",
            "loss: 1.420458 [  192/ 1864]\n",
            "31.0\n",
            "loss: 1.514208 [  256/ 1864]\n",
            "25.0\n",
            "loss: 1.498583 [  320/ 1864]\n",
            "26.0\n",
            "loss: 1.482958 [  384/ 1864]\n",
            "27.0\n",
            "loss: 1.514208 [  448/ 1864]\n",
            "25.0\n",
            "loss: 1.467333 [  512/ 1864]\n",
            "28.0\n",
            "loss: 1.545458 [  576/ 1864]\n",
            "23.0\n",
            "loss: 1.436083 [  640/ 1864]\n",
            "30.0\n",
            "loss: 1.467333 [  704/ 1864]\n",
            "28.0\n",
            "loss: 1.467333 [  768/ 1864]\n",
            "28.0\n",
            "loss: 1.514208 [  832/ 1864]\n",
            "25.0\n",
            "loss: 1.561083 [  896/ 1864]\n",
            "22.0\n",
            "loss: 1.529833 [  960/ 1864]\n",
            "24.0\n",
            "loss: 1.420458 [ 1024/ 1864]\n",
            "31.0\n",
            "loss: 1.467333 [ 1088/ 1864]\n",
            "28.0\n",
            "loss: 1.498582 [ 1152/ 1864]\n",
            "26.0\n",
            "loss: 1.498583 [ 1216/ 1864]\n",
            "26.0\n",
            "loss: 1.420458 [ 1280/ 1864]\n",
            "31.0\n",
            "loss: 1.561083 [ 1344/ 1864]\n",
            "22.0\n",
            "loss: 1.420458 [ 1408/ 1864]\n",
            "31.0\n",
            "loss: 1.357958 [ 1472/ 1864]\n",
            "35.0\n",
            "loss: 1.389208 [ 1536/ 1864]\n",
            "33.0\n",
            "loss: 1.420458 [ 1600/ 1864]\n",
            "31.0\n",
            "loss: 1.467333 [ 1664/ 1864]\n",
            "28.0\n",
            "loss: 1.436083 [ 1728/ 1864]\n",
            "30.0\n",
            "loss: 1.514208 [ 1792/ 1864]\n",
            "25.0\n",
            "loss: 1.529833 [  232/ 1864]\n",
            "3.0\n",
            "Test Error: \n",
            " Accuracy: 42.9%, Avg loss: 0.025243 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.561083 [    0/ 1864]\n",
            "22.0\n",
            "loss: 1.482958 [   64/ 1864]\n",
            "27.0\n",
            "loss: 1.326708 [  128/ 1864]\n",
            "37.0\n",
            "loss: 1.498583 [  192/ 1864]\n",
            "26.0\n",
            "loss: 1.467333 [  256/ 1864]\n",
            "28.0\n",
            "loss: 1.529833 [  320/ 1864]\n",
            "24.0\n",
            "loss: 1.436083 [  384/ 1864]\n",
            "30.0\n",
            "loss: 1.451708 [  448/ 1864]\n",
            "29.0\n",
            "loss: 1.436083 [  512/ 1864]\n",
            "30.0\n",
            "loss: 1.529833 [  576/ 1864]\n",
            "24.0\n",
            "loss: 1.482958 [  640/ 1864]\n",
            "27.0\n",
            "loss: 1.545458 [  704/ 1864]\n",
            "23.0\n",
            "loss: 1.576708 [  768/ 1864]\n",
            "21.0\n",
            "loss: 1.436083 [  832/ 1864]\n",
            "30.0\n",
            "loss: 1.420458 [  896/ 1864]\n",
            "31.0\n",
            "loss: 1.467333 [  960/ 1864]\n",
            "28.0\n",
            "loss: 1.451708 [ 1024/ 1864]\n",
            "29.0\n",
            "loss: 1.436083 [ 1088/ 1864]\n",
            "30.0\n",
            "loss: 1.498583 [ 1152/ 1864]\n",
            "26.0\n",
            "loss: 1.529833 [ 1216/ 1864]\n",
            "24.0\n",
            "loss: 1.545458 [ 1280/ 1864]\n",
            "23.0\n",
            "loss: 1.420458 [ 1344/ 1864]\n",
            "31.0\n",
            "loss: 1.451708 [ 1408/ 1864]\n",
            "29.0\n",
            "loss: 1.404833 [ 1472/ 1864]\n",
            "32.0\n",
            "loss: 1.436083 [ 1536/ 1864]\n",
            "30.0\n",
            "loss: 1.467333 [ 1600/ 1864]\n",
            "28.0\n",
            "loss: 1.482958 [ 1664/ 1864]\n",
            "27.0\n",
            "loss: 1.576708 [ 1728/ 1864]\n",
            "21.0\n",
            "loss: 1.529833 [ 1792/ 1864]\n",
            "24.0\n",
            "loss: 1.279832 [  232/ 1864]\n",
            "5.0\n",
            "Test Error: \n",
            " Accuracy: 42.9%, Avg loss: 0.025431 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.529833 [    0/ 1864]\n",
            "24.0\n",
            "loss: 1.404833 [   64/ 1864]\n",
            "32.0\n",
            "loss: 1.529833 [  128/ 1864]\n",
            "24.0\n",
            "loss: 1.529833 [  192/ 1864]\n",
            "24.0\n",
            "loss: 1.404833 [  256/ 1864]\n",
            "32.0\n",
            "loss: 1.373583 [  320/ 1864]\n",
            "34.0\n",
            "loss: 1.451708 [  384/ 1864]\n",
            "29.0\n",
            "loss: 1.389208 [  448/ 1864]\n",
            "33.0\n",
            "loss: 1.451708 [  512/ 1864]\n",
            "29.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-1bb7d505b381>\u001b[0m in \u001b[0;36m<cell line: 79>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m   \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m   \u001b[0mtest_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-1bb7d505b381>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# print(y.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def label_changer(y):\n",
        "  for i in range(len(y)):\n",
        "    y[i] = int(y[i])\n",
        "    if y[i] == 3 or y[i] == 5:\n",
        "      y[i] = 0\n",
        "    elif y[i] != 4:\n",
        "      y[i] += 1\n",
        "    num = y[i]\n",
        "    y[i] = torch.tensor([[0, 0, 0, 0, 0]], dtype=torch.float32)\n",
        "    y[i][0][num] = float(1)\n",
        "  y = torch.cat(y, dim=0)\n",
        "  return y\n",
        "from re import I\n",
        "# label 0, 1, 2, 4 (Diabetic retinopathy)\n",
        "# og labels\n",
        "#     0: \"Mild Diabetic Nonproliferative Retinopathy\",\n",
        "#     1: \"Moderate Diabetic Nonproliferative Retinopathy\",\n",
        "#     2: \"Severe Diabetic Nonproliferative Retinopathy\",\n",
        "#     3: \"Hypertensive Retinopathy\",\n",
        "#     4: \"Proliferative Diabetic Retinopathy\",\n",
        "#     5: \"No Retinopathy\"\n",
        "# new labels\n",
        "#     0: 3, 5\n",
        "#     1: 0\n",
        "#     2: 1\n",
        "#     3: 2\n",
        "#     4: 4\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)\n",
        "# model.summary()\n",
        "batch_size = 64\n",
        "# epochs = 5\n",
        "loss_fn =nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    optimizer.zero_grad()\n",
        "  # Compute prediction and loss\n",
        "    X = X.cpu()\n",
        "    #y.expand(-1, 4)\n",
        "    y = list(y)\n",
        "    y = label_changer(y)\n",
        "    y = y.cpu()\n",
        "    pred = model(X)\n",
        "    pred = pred.to(torch.float32)\n",
        "    y = y.to(torch.float32)\n",
        "    # print(y.shape)\n",
        "    loss = loss_fn(pred, y.argmax(1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss, current = loss.item(), batch * len(X)\n",
        "    # print(y)\n",
        "    # print(pred)\n",
        "    # print(pred.argmax(1))\n",
        "    # print('pred', pred)\n",
        "    # print('y', y)\n",
        "    accuracy = (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
        "    print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "    print(accuracy)\n",
        "    # print('---')\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "      X = X.cpu()\n",
        "      y = list(y)\n",
        "      y = label_changer(y)\n",
        "      y = y.cpu()\n",
        "      pred = model(X)\n",
        "      test_loss += loss_fn(pred, y.argmax(1)).item()\n",
        "      correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "epochs = 20\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "  test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# label 0: Mild Diabetic Nonproliferative Retinopathy\n",
        "\n",
        "def label_changer2(y):\n",
        "  for i in range(len(y)):\n",
        "    y[i] = int(y[i])\n",
        "    if y[i] == 0:\n",
        "      y[i] = 1\n",
        "    else:\n",
        "      y[i] = 0\n",
        "    num = y[i]\n",
        "    y[i] = torch.tensor([[0, 0]], dtype=torch.float32)\n",
        "    y[i][0][num] = float(1)\n",
        "  y = torch.cat(y, dim=0)\n",
        "  # print(y)\n",
        "  return y\n",
        "model2 = NeuralNetwork2().to(device)\n",
        "batch_size = 64\n",
        "loss_fn =nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model2.parameters(), lr=0.001)\n",
        "def train_loop(dataloader, model2, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X = X.cpu()\n",
        "    y = list(y)\n",
        "    y = label_changer2(y)\n",
        "    y = y.cpu()\n",
        "    pred = model2(X)\n",
        "    loss = loss_fn(pred, y.argmax(1))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss, current = loss.item(), batch * len(X)\n",
        "    print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "def test_loop(dataloader, model2, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X = X.cpu()\n",
        "      y = list(y)\n",
        "      y = label_changer2(y)\n",
        "      y = y.cpu()\n",
        "      pred = model2(X)\n",
        "      test_loss += loss_fn(pred, y.argmax(1)).item()\n",
        "      correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
        "      print(pred.argmax(1))\n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "epochs = 7\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  train_loop(train_dataloader, model2, loss_fn, optimizer)\n",
        "  test_loop(test_dataloader, model2, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "RrM6e1o4d3xx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc20e7a5-c9d4-4c9f-fc0a-79a05242b64d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.690829 [    0/ 1864]\n",
            "loss: 4.086617 [   64/ 1864]\n",
            "loss: 2.495634 [  128/ 1864]\n",
            "loss: 0.618161 [  192/ 1864]\n",
            "loss: 5.712082 [  256/ 1864]\n",
            "loss: 1.707914 [  320/ 1864]\n",
            "loss: 2.083668 [  384/ 1864]\n",
            "loss: 1.572575 [  448/ 1864]\n",
            "loss: 1.180959 [  512/ 1864]\n",
            "loss: 1.066002 [  576/ 1864]\n",
            "loss: 0.571843 [  640/ 1864]\n",
            "loss: 1.601277 [  704/ 1864]\n",
            "loss: 1.191836 [  768/ 1864]\n",
            "loss: 0.561246 [  832/ 1864]\n",
            "loss: 0.511528 [  896/ 1864]\n",
            "loss: 1.198717 [  960/ 1864]\n",
            "loss: 1.093388 [ 1024/ 1864]\n",
            "loss: 1.197170 [ 1088/ 1864]\n",
            "loss: 0.806767 [ 1152/ 1864]\n",
            "loss: 0.485874 [ 1216/ 1864]\n",
            "loss: 0.503896 [ 1280/ 1864]\n",
            "loss: 0.658843 [ 1344/ 1864]\n",
            "loss: 0.725529 [ 1408/ 1864]\n",
            "loss: 0.745004 [ 1472/ 1864]\n",
            "loss: 0.675302 [ 1536/ 1864]\n",
            "loss: 0.543743 [ 1600/ 1864]\n",
            "loss: 0.568277 [ 1664/ 1864]\n",
            "loss: 0.642776 [ 1728/ 1864]\n",
            "loss: 0.484085 [ 1792/ 1864]\n",
            "loss: 0.922915 [  232/ 1864]\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Test Error: \n",
            " Accuracy: 76.4%, Avg loss: 0.010418 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.646971 [    0/ 1864]\n",
            "loss: 0.843300 [   64/ 1864]\n",
            "loss: 0.596534 [  128/ 1864]\n",
            "loss: 0.460734 [  192/ 1864]\n",
            "loss: 0.494252 [  256/ 1864]\n",
            "loss: 0.584049 [  320/ 1864]\n",
            "loss: 0.522183 [  384/ 1864]\n",
            "loss: 0.571033 [  448/ 1864]\n",
            "loss: 0.612581 [  512/ 1864]\n",
            "loss: 0.627386 [  576/ 1864]\n",
            "loss: 0.540158 [  640/ 1864]\n",
            "loss: 0.607734 [  704/ 1864]\n",
            "loss: 0.524292 [  768/ 1864]\n",
            "loss: 0.547865 [  832/ 1864]\n",
            "loss: 0.571603 [  896/ 1864]\n",
            "loss: 0.499900 [  960/ 1864]\n",
            "loss: 0.601475 [ 1024/ 1864]\n",
            "loss: 0.568948 [ 1088/ 1864]\n",
            "loss: 0.569684 [ 1152/ 1864]\n",
            "loss: 0.683222 [ 1216/ 1864]\n",
            "loss: 0.481822 [ 1280/ 1864]\n",
            "loss: 0.654797 [ 1344/ 1864]\n",
            "loss: 0.485180 [ 1408/ 1864]\n",
            "loss: 0.466123 [ 1472/ 1864]\n",
            "loss: 0.539266 [ 1536/ 1864]\n",
            "loss: 0.559280 [ 1600/ 1864]\n",
            "loss: 0.441650 [ 1664/ 1864]\n",
            "loss: 0.511972 [ 1728/ 1864]\n",
            "loss: 0.635770 [ 1792/ 1864]\n",
            "loss: 0.228699 [  232/ 1864]\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Test Error: \n",
            " Accuracy: 76.4%, Avg loss: 0.010144 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.460384 [    0/ 1864]\n",
            "loss: 0.668368 [   64/ 1864]\n",
            "loss: 0.489575 [  128/ 1864]\n",
            "loss: 0.470789 [  192/ 1864]\n",
            "loss: 0.514324 [  256/ 1864]\n",
            "loss: 0.461774 [  320/ 1864]\n",
            "loss: 0.518640 [  384/ 1864]\n",
            "loss: 0.540585 [  448/ 1864]\n",
            "loss: 0.627325 [  512/ 1864]\n",
            "loss: 0.542098 [  576/ 1864]\n",
            "loss: 0.591964 [  640/ 1864]\n",
            "loss: 0.484505 [  704/ 1864]\n",
            "loss: 0.639705 [  768/ 1864]\n",
            "loss: 0.554324 [  832/ 1864]\n",
            "loss: 0.578702 [  896/ 1864]\n",
            "loss: 0.584679 [  960/ 1864]\n",
            "loss: 0.528585 [ 1024/ 1864]\n",
            "loss: 0.508921 [ 1088/ 1864]\n",
            "loss: 0.528624 [ 1152/ 1864]\n",
            "loss: 0.458084 [ 1216/ 1864]\n",
            "loss: 0.579646 [ 1280/ 1864]\n",
            "loss: 0.546863 [ 1344/ 1864]\n",
            "loss: 0.542639 [ 1408/ 1864]\n",
            "loss: 0.654453 [ 1472/ 1864]\n",
            "loss: 0.528471 [ 1536/ 1864]\n",
            "loss: 0.540751 [ 1600/ 1864]\n",
            "loss: 0.622645 [ 1664/ 1864]\n",
            "loss: 0.609250 [ 1728/ 1864]\n",
            "loss: 0.515261 [ 1792/ 1864]\n",
            "loss: 0.423293 [  232/ 1864]\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Test Error: \n",
            " Accuracy: 76.4%, Avg loss: 0.009359 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.492128 [    0/ 1864]\n",
            "loss: 0.447734 [   64/ 1864]\n",
            "loss: 0.557455 [  128/ 1864]\n",
            "loss: 0.488346 [  192/ 1864]\n",
            "loss: 0.474845 [  256/ 1864]\n",
            "loss: 0.702483 [  320/ 1864]\n",
            "loss: 0.452400 [  384/ 1864]\n",
            "loss: 0.485893 [  448/ 1864]\n",
            "loss: 0.611364 [  512/ 1864]\n",
            "loss: 0.556973 [  576/ 1864]\n",
            "loss: 0.600813 [  640/ 1864]\n",
            "loss: 0.533606 [  704/ 1864]\n",
            "loss: 0.476792 [  768/ 1864]\n",
            "loss: 0.615649 [  832/ 1864]\n",
            "loss: 0.572057 [  896/ 1864]\n",
            "loss: 0.541262 [  960/ 1864]\n",
            "loss: 0.518483 [ 1024/ 1864]\n",
            "loss: 0.462589 [ 1088/ 1864]\n",
            "loss: 0.570410 [ 1152/ 1864]\n",
            "loss: 0.532152 [ 1216/ 1864]\n",
            "loss: 0.621796 [ 1280/ 1864]\n",
            "loss: 0.515643 [ 1344/ 1864]\n",
            "loss: 0.561734 [ 1408/ 1864]\n",
            "loss: 0.545941 [ 1472/ 1864]\n",
            "loss: 0.583787 [ 1536/ 1864]\n",
            "loss: 0.551055 [ 1600/ 1864]\n",
            "loss: 0.569596 [ 1664/ 1864]\n",
            "loss: 0.500065 [ 1728/ 1864]\n",
            "loss: 0.523870 [ 1792/ 1864]\n",
            "loss: 0.426866 [  232/ 1864]\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Test Error: \n",
            " Accuracy: 76.4%, Avg loss: 0.009420 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.530051 [    0/ 1864]\n",
            "loss: 0.628405 [   64/ 1864]\n",
            "loss: 0.509686 [  128/ 1864]\n",
            "loss: 0.467135 [  192/ 1864]\n",
            "loss: 0.540439 [  256/ 1864]\n",
            "loss: 0.466097 [  320/ 1864]\n",
            "loss: 0.601192 [  384/ 1864]\n",
            "loss: 0.484918 [  448/ 1864]\n",
            "loss: 0.589327 [  512/ 1864]\n",
            "loss: 0.485570 [  576/ 1864]\n",
            "loss: 0.631672 [  640/ 1864]\n",
            "loss: 0.443794 [  704/ 1864]\n",
            "loss: 0.593880 [  768/ 1864]\n",
            "loss: 0.653083 [  832/ 1864]\n",
            "loss: 0.433635 [  896/ 1864]\n",
            "loss: 0.536051 [  960/ 1864]\n",
            "loss: 0.535442 [ 1024/ 1864]\n",
            "loss: 0.555394 [ 1088/ 1864]\n",
            "loss: 0.543442 [ 1152/ 1864]\n",
            "loss: 0.562782 [ 1216/ 1864]\n",
            "loss: 0.536964 [ 1280/ 1864]\n",
            "loss: 0.492435 [ 1344/ 1864]\n",
            "loss: 0.562762 [ 1408/ 1864]\n",
            "loss: 0.456253 [ 1472/ 1864]\n",
            "loss: 0.555087 [ 1536/ 1864]\n",
            "loss: 0.595712 [ 1600/ 1864]\n",
            "loss: 0.446484 [ 1664/ 1864]\n",
            "loss: 0.517762 [ 1728/ 1864]\n",
            "loss: 0.447404 [ 1792/ 1864]\n",
            "loss: 0.278581 [  232/ 1864]\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Test Error: \n",
            " Accuracy: 76.4%, Avg loss: 0.010161 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.529344 [    0/ 1864]\n",
            "loss: 0.626601 [   64/ 1864]\n",
            "loss: 0.561006 [  128/ 1864]\n",
            "loss: 0.543790 [  192/ 1864]\n",
            "loss: 0.407996 [  256/ 1864]\n",
            "loss: 0.561152 [  320/ 1864]\n",
            "loss: 0.596713 [  384/ 1864]\n",
            "loss: 0.488625 [  448/ 1864]\n",
            "loss: 0.468569 [  512/ 1864]\n",
            "loss: 0.497097 [  576/ 1864]\n",
            "loss: 0.523263 [  640/ 1864]\n",
            "loss: 0.656331 [  704/ 1864]\n",
            "loss: 0.548857 [  768/ 1864]\n",
            "loss: 0.513212 [  832/ 1864]\n",
            "loss: 0.520542 [  896/ 1864]\n",
            "loss: 0.554268 [  960/ 1864]\n",
            "loss: 0.445211 [ 1024/ 1864]\n",
            "loss: 0.606799 [ 1088/ 1864]\n",
            "loss: 0.396515 [ 1152/ 1864]\n",
            "loss: 0.533666 [ 1216/ 1864]\n",
            "loss: 0.533983 [ 1280/ 1864]\n",
            "loss: 0.598529 [ 1344/ 1864]\n",
            "loss: 0.674377 [ 1408/ 1864]\n",
            "loss: 0.520809 [ 1472/ 1864]\n",
            "loss: 0.544216 [ 1536/ 1864]\n",
            "loss: 0.502440 [ 1600/ 1864]\n",
            "loss: 0.503069 [ 1664/ 1864]\n",
            "loss: 0.471447 [ 1728/ 1864]\n",
            "loss: 0.526918 [ 1792/ 1864]\n",
            "loss: 0.434626 [  232/ 1864]\n",
            "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Test Error: \n",
            " Accuracy: 76.8%, Avg loss: 0.009478 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.522189 [    0/ 1864]\n",
            "loss: 0.640714 [   64/ 1864]\n",
            "loss: 0.623592 [  128/ 1864]\n",
            "loss: 0.502707 [  192/ 1864]\n",
            "loss: 0.577368 [  256/ 1864]\n",
            "loss: 0.634225 [  320/ 1864]\n",
            "loss: 0.580494 [  384/ 1864]\n",
            "loss: 0.481502 [  448/ 1864]\n",
            "loss: 0.527855 [  512/ 1864]\n",
            "loss: 0.499872 [  576/ 1864]\n",
            "loss: 0.487661 [  640/ 1864]\n",
            "loss: 0.499243 [  704/ 1864]\n",
            "loss: 0.566743 [  768/ 1864]\n",
            "loss: 0.509023 [  832/ 1864]\n",
            "loss: 0.579935 [  896/ 1864]\n",
            "loss: 0.538565 [  960/ 1864]\n",
            "loss: 0.600132 [ 1024/ 1864]\n",
            "loss: 0.565064 [ 1088/ 1864]\n",
            "loss: 0.462963 [ 1152/ 1864]\n",
            "loss: 0.575161 [ 1216/ 1864]\n",
            "loss: 0.449524 [ 1280/ 1864]\n",
            "loss: 0.523282 [ 1344/ 1864]\n",
            "loss: 0.470475 [ 1408/ 1864]\n",
            "loss: 0.519861 [ 1472/ 1864]\n",
            "loss: 0.474199 [ 1536/ 1864]\n",
            "loss: 0.492947 [ 1600/ 1864]\n",
            "loss: 0.453145 [ 1664/ 1864]\n",
            "loss: 0.370110 [ 1728/ 1864]\n",
            "loss: 0.509857 [ 1792/ 1864]\n",
            "loss: 0.403969 [  232/ 1864]\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0])\n",
            "Test Error: \n",
            " Accuracy: 76.0%, Avg loss: 0.012116 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model2.state_dict(),  \"binary0model\")"
      ],
      "metadata": {
        "id": "SW5Sj8z_fOeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# label 1: Moderate Diabetic Nonproliferative Retinopathy\n",
        "\n",
        "def label_changer2(y):\n",
        "  for i in range(len(y)):\n",
        "    y[i] = int(y[i])\n",
        "    if y[i] == 1:\n",
        "      y[i] = 1\n",
        "    else:\n",
        "      y[i] = 0\n",
        "    num = y[i]\n",
        "    y[i] = torch.tensor([[0, 0]], dtype=torch.float32)\n",
        "    y[i][0][num] = float(1)\n",
        "  y = torch.cat(y, dim=0)\n",
        "  # print(y)\n",
        "  return y\n",
        "model2 = NeuralNetwork2().to(device)\n",
        "batch_size = 64\n",
        "loss_fn =nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model2.parameters(), lr=0.001)\n",
        "def train_loop(dataloader, model2, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X = X.cpu()\n",
        "    y = list(y)\n",
        "    y = label_changer2(y)\n",
        "    y = y.cpu()\n",
        "    pred = model2(X)\n",
        "    loss = loss_fn(pred, y.argmax(1))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss, current = loss.item(), batch * len(X)\n",
        "    print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "def test_loop(dataloader, model2, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X = X.cpu()\n",
        "      y = list(y)\n",
        "      y = label_changer2(y)\n",
        "      y = y.cpu()\n",
        "      pred = model2(X)\n",
        "      test_loss += loss_fn(pred, y.argmax(1)).item()\n",
        "      correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "epochs = 7\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  train_loop(train_dataloader, model2, loss_fn, optimizer)\n",
        "  test_loop(test_dataloader, model2, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "e8Fl_65Vd88B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model2.state_dict(),  \"binary1model\")"
      ],
      "metadata": {
        "id": "LIhdpflLfIgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# label 2: Severe Diabetic Nonproliferative Retinopathy\n",
        "\n",
        "def label_changer2(y):\n",
        "  for i in range(len(y)):\n",
        "    y[i] = int(y[i])\n",
        "    if y[i] == 2:\n",
        "      y[i] = 1\n",
        "    else:\n",
        "      y[i] = 0\n",
        "    num = y[i]\n",
        "    y[i] = torch.tensor([[0, 0]], dtype=torch.float32)\n",
        "    y[i][0][num] = float(1)\n",
        "  y = torch.cat(y, dim=0)\n",
        "  # print(y)\n",
        "  return y\n",
        "model2 = NeuralNetwork2().to(device)\n",
        "batch_size = 64\n",
        "loss_fn =nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model2.parameters(), lr=0.001)\n",
        "def train_loop(dataloader, model2, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X = X.cpu()\n",
        "    y = list(y)\n",
        "    y = label_changer2(y)\n",
        "    y = y.cpu()\n",
        "    pred = model2(X)\n",
        "    loss = loss_fn(pred, y.argmax(1))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss, current = loss.item(), batch * len(X)\n",
        "    print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "def test_loop(dataloader, model2, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X = X.cpu()\n",
        "      y = list(y)\n",
        "      y = label_changer2(y)\n",
        "      y = y.cpu()\n",
        "      pred = model2(X)\n",
        "      test_loss += loss_fn(pred, y.argmax(1)).item()\n",
        "      correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "epochs = 7\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  train_loop(train_dataloader, model2, loss_fn, optimizer)\n",
        "  test_loop(test_dataloader, model2, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "hUewMTcscp0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model2.state_dict(),  \"binary2model\")"
      ],
      "metadata": {
        "id": "dw5FXRRCfF98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLexeoXcLylg"
      },
      "outputs": [],
      "source": [
        "# label 3: Hypertensive  Retinopathy\n",
        "\n",
        "def label_changer2(y):\n",
        "  for i in range(len(y)):\n",
        "    y[i] = int(y[i])\n",
        "    if y[i] == 3:\n",
        "      y[i] = 1\n",
        "    else:\n",
        "      y[i] = 0\n",
        "    num = y[i]\n",
        "    y[i] = torch.tensor([[0, 0]], dtype=torch.float32)\n",
        "    y[i][0][num] = float(1)\n",
        "  y = torch.cat(y, dim=0)\n",
        "  # print(y)\n",
        "  return y\n",
        "model2 = NeuralNetwork2().to(device)\n",
        "batch_size = 64\n",
        "loss_fn =nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model2.parameters(), lr=0.001)\n",
        "def train_loop(dataloader, model2, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X = X.cpu()\n",
        "    y = list(y)\n",
        "    y = label_changer2(y)\n",
        "    y = y.cpu()\n",
        "    pred = model2(X)\n",
        "    loss = loss_fn(pred, y.argmax(1))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss, current = loss.item(), batch * len(X)\n",
        "    print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "def test_loop(dataloader, model2, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X = X.cpu()\n",
        "      y = list(y)\n",
        "      y = label_changer2(y)\n",
        "      y = y.cpu()\n",
        "      pred = model2(X)\n",
        "      test_loss += loss_fn(pred, y.argmax(1)).item()\n",
        "      correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "epochs = 7\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  train_loop(train_dataloader, model2, loss_fn, optimizer)\n",
        "  test_loop(test_dataloader, model2, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model2.state_dict(),  \"binary3model\")"
      ],
      "metadata": {
        "id": "wAwfR61Ee_5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# label 4: Severe Diabetic Nonproliferative Retinopathy\n",
        "\n",
        "def label_changer2(y):\n",
        "  for i in range(len(y)):\n",
        "    y[i] = int(y[i])\n",
        "    if y[i] == 4:\n",
        "      y[i] = 1\n",
        "    else:\n",
        "      y[i] = 0\n",
        "    num = y[i]\n",
        "    y[i] = torch.tensor([[0, 0]], dtype=torch.float32)\n",
        "    y[i][0][num] = float(1)\n",
        "  y = torch.cat(y, dim=0)\n",
        "  # print(y)\n",
        "  return y\n",
        "model2 = NeuralNetwork2().to(device)\n",
        "batch_size = 64\n",
        "loss_fn =nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model2.parameters(), lr=0.001)\n",
        "def train_loop(dataloader, model2, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X = X.cpu()\n",
        "    y = list(y)\n",
        "    y = label_changer2(y)\n",
        "    y = y.cpu()\n",
        "    pred = model2(X)\n",
        "    loss = loss_fn(pred, y.argmax(1))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss, current = loss.item(), batch * len(X)\n",
        "    print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "def test_loop(dataloader, model2, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X = X.cpu()\n",
        "      y = list(y)\n",
        "      y = label_changer2(y)\n",
        "      y = y.cpu()\n",
        "      pred = model2(X)\n",
        "      test_loss += loss_fn(pred, y.argmax(1)).item()\n",
        "      correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "epochs = 7\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  train_loop(train_dataloader, model2, loss_fn, optimizer)\n",
        "  test_loop(test_dataloader, model2, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "6_g5Kt4-d_xR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model2.state_dict(),  \"binary4model\")"
      ],
      "metadata": {
        "id": "1u-ijRihBmjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collapsed Commented Out"
      ],
      "metadata": {
        "id": "Eedcdmz56TpX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a46WE0x5LxIt"
      },
      "outputs": [],
      "source": [
        "# # label 1\n",
        "# def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "#   size = len(dataloader.dataset)\n",
        "#   for batch, (X, y) in enumerate(dataloader):\n",
        "#   # Compute prediction and loss\n",
        "#     X = X.cpu()\n",
        "#     y = list(y)\n",
        "#     for i in range(len(y)):\n",
        "#       y[i] = int(y[i])\n",
        "#       if y[i] == 1:\n",
        "#         y[i] = 1\n",
        "#       else:\n",
        "#         y[i] = 0\n",
        "#     y = torch.tensor(y)\n",
        "#     y = y.cpu()\n",
        "#     pred = model(X)\n",
        "#     loss = loss_fn(pred, y)\n",
        "#     # Backpropagation\n",
        "#     optimizer.zero_grad()\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     if batch % 100 == 0:\n",
        "#       loss, current = loss.item(), batch * len(X)\n",
        "#       print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "# def test_loop(dataloader, model, loss_fn):\n",
        "#   size = len(dataloader.dataset)\n",
        "#   test_loss, correct = 0, 0\n",
        "#   with torch.no_grad():\n",
        "#     for X, y in dataloader:\n",
        "#       X = X.cpu()\n",
        "#       y = list(y)\n",
        "#       for i in range(len(y)):\n",
        "#         y[i] = int(y[i])\n",
        "#         if y[i] == 1:\n",
        "#           y[i] = 1\n",
        "#         else:\n",
        "#           y[i] = 0\n",
        "#       y = torch.tensor(y)\n",
        "#       y = y.cpu()\n",
        "#       pred = model(X)\n",
        "#       test_loss += loss_fn(pred, y).item()\n",
        "#       correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "#     test_loss /= size # (TP + TN)/(TP + TN + FP + FN) i think\n",
        "#     correct /= size\n",
        "#     print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "# epochs = 7\n",
        "# for t in range(epochs):\n",
        "#   print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "#   train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "#   test_loop(test_dataloader, model, loss_fn)\n",
        "# print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48mEyce0Lx_s"
      },
      "outputs": [],
      "source": [
        "# # label 2\n",
        "# def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "#   size = len(dataloader.dataset)\n",
        "#   for batch, (X, y) in enumerate(dataloader):\n",
        "#   # Compute prediction and loss\n",
        "#     X = X.cpu()\n",
        "#     y = list(y)\n",
        "#     for i in range(len(y)):\n",
        "#       y[i] = int(y[i])\n",
        "#       if y[i] == 2:\n",
        "#         y[i] = 1\n",
        "#       else:\n",
        "#         y[i] = 0\n",
        "#     y = torch.tensor(y)\n",
        "#     y = y.cpu()\n",
        "#     pred = model(X)\n",
        "#     loss = loss_fn(pred, y)\n",
        "#     # Backpropagation\n",
        "#     optimizer.zero_grad()\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     if batch % 100 == 0:\n",
        "#       loss, current = loss.item(), batch * len(X)\n",
        "#       print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "# def test_loop(dataloader, model, loss_fn):\n",
        "#   size = len(dataloader.dataset)\n",
        "#   test_loss, correct = 0, 0\n",
        "#   with torch.no_grad():\n",
        "#     for X, y in dataloader:\n",
        "#       X = X.cpu()\n",
        "#       y = list(y)\n",
        "#       for i in range(len(y)):\n",
        "#         y[i] = int(y[i])\n",
        "#         if y[i] == 2:\n",
        "#           y[i] = 1\n",
        "#         else:\n",
        "#           y[i] = 0\n",
        "#       y = torch.tensor(y)\n",
        "#       y = y.cpu()\n",
        "#       pred = model(X)\n",
        "#       test_loss += loss_fn(pred, y).item()\n",
        "#       correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "#     test_loss /= size # (TP + TN)/(TP + TN + FP + FN) i think\n",
        "#     correct /= size\n",
        "#     print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "# epochs = 7\n",
        "# for t in range(epochs):\n",
        "#   print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "#   train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "#   test_loop(test_dataloader, model, loss_fn)\n",
        "# print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZp4oEsfLzE_"
      },
      "outputs": [],
      "source": [
        "# # label 4\n",
        "# def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "#   size = len(dataloader.dataset)\n",
        "#   for batch, (X, y) in enumerate(dataloader):\n",
        "#   # Compute prediction and loss\n",
        "#     X = X.cpu()\n",
        "#     y = list(y)\n",
        "#     for i in range(len(y)):\n",
        "#       y[i] = int(y[i])\n",
        "#       if y[i] == 4:\n",
        "#         y[i] = 1\n",
        "#       else:\n",
        "#         y[i] = 0\n",
        "#     y = torch.tensor(y)\n",
        "#     y = y.cpu()\n",
        "#     pred = model(X)\n",
        "#     loss = loss_fn(pred, y)\n",
        "#     # Backpropagation\n",
        "#     optimizer.zero_grad()\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     if batch % 100 == 0:\n",
        "#       loss, current = loss.item(), batch * len(X)\n",
        "#       print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "# def test_loop(dataloader, model, loss_fn):\n",
        "#   size = len(dataloader.dataset)\n",
        "#   test_loss, correct = 0, 0\n",
        "#   with torch.no_grad():\n",
        "#     for X, y in dataloader:\n",
        "#       X = X.cpu()\n",
        "#       y = list(y)\n",
        "#       for i in range(len(y)):\n",
        "#         y[i] = int(y[i])\n",
        "#         if y[i] == 4:\n",
        "#           y[i] = 1\n",
        "#         else:\n",
        "#           y[i] = 0\n",
        "#       y = torch.tensor(y)\n",
        "#       y = y.cpu()\n",
        "#       pred = model(X)\n",
        "#       test_loss += loss_fn(pred, y).item()\n",
        "#       correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "#     test_loss /= size # (TP + TN)/(TP + TN + FP + FN) i think\n",
        "#     correct /= size\n",
        "#     print(correct)\n",
        "#     print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "# epochs = 7\n",
        "# for t in range(epochs):\n",
        "#   print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "#   train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "#   test_loop(test_dataloader, model, loss_fn)\n",
        "# print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FElJ-xT_rlUk"
      },
      "outputs": [],
      "source": [
        "# old data proccessing code - BAD\n",
        "# import numpy as np\n",
        "# import cv2\n",
        "# from torch.utils.data import DataLoader\n",
        "# from tqdm import tqdm\n",
        "# global img_path\n",
        "# global input_image\n",
        "# global input_tensor\n",
        "# global input_batch\n",
        "# train_dict = dict()\n",
        "# test_dict = dict()\n",
        "# val_dict = dict()\n",
        "# class RetinoDataset(Dataset):\n",
        "#   def __init__(self):\n",
        "#     self.train_set = []\n",
        "#     self.test_set = []\n",
        "#     self.validation_set = []\n",
        "#     for i in range(0, 6):\n",
        "#       path = f\"/content/drive/MyDrive/TrainingData_Final/{i}\"\n",
        "#       all_imgs_0 = os.listdir(path)\n",
        "#       # /content/drive/MyDrive/TrainingData_Final/0\n",
        "#       all_imgs_0 = [f\"{path}/{_path}\" for _path in all_imgs_0]\n",
        "#       num_imgs_0 = len(all_imgs_0)\n",
        "#       arr = np.arange(num_imgs_0)\n",
        "#       np.random.shuffle(arr)\n",
        "#       self.train_set += [all_imgs_0[idx] for idx in arr[0:int(0.05*num_imgs_0)]]\n",
        "#       self.test_set += [all_imgs_0[idx]  for idx in arr[int(0.05*num_imgs_0):int(0.9*num_imgs_0)]]\n",
        "#       self.validation_set += [all_imgs_0[idx]  for idx in arr[int(0.9*num_imgs_0):int(num_imgs_0)]]\n",
        "#       # print(self.train_set)\n",
        "#     # self.__len__()\n",
        "#     # self.main()\n",
        "#     # self.__getitem()\n",
        "\n",
        "\n",
        "\n",
        "#     self.train_data = self.train_set\n",
        "#     self.test_data = self.test_set\n",
        "#     self.val_data = self.validation_set\n",
        "\n",
        "#       #transpose\n",
        "#     self.preprocess = transforms.Compose([\n",
        "#         transforms.Resize((256, 256)),\n",
        "#         transforms.ToTensor(),\n",
        "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "#     ])\n",
        "\n",
        "#   # def __len__(self):\n",
        "#     # return self.train_data\n",
        "#     # return len(self.test_data)\n",
        "#     # return len(self.val_data)\n",
        "#     # return(self.data_sets)\n",
        "#   # global img_path\n",
        "#   # global input_image\n",
        "#   # global input_tensor\n",
        "#   # global input_batch\n",
        "#   def __getitem(self, curr_dataset, idx):\n",
        "#     # print('idx:', idx)\n",
        "#     # print('len:', len(idx))\n",
        "#     # print(curr_dataset)\n",
        "#     img_path = idx[0]\n",
        "#     input_image = Image.open(img_path)\n",
        "#     input_tensor = self.preprocess(input_image)\n",
        "#     input_batch = input_tensor.unsqueeze(0)\n",
        "#     # print(input_batch)\n",
        "\n",
        "#     return {\"image\": input_batch, \"label\": int(img_path[42])}\n",
        "\n",
        "#   def main(self):\n",
        "#     train_dataloader = DataLoader(self.train_data, batch_size=1)\n",
        "#     test_dataloader = DataLoader(self.test_data, batch_size=1)\n",
        "#     val_dataloader = DataLoader(self.val_data, batch_size=1)\n",
        "#     # print(train_dataloader)\n",
        "\n",
        "#     for img in tqdm(train_dataloader):\n",
        "\n",
        "#       # print(img[\"image\"].shape)\n",
        "#       train_dict[] = (self.__getitem(self.train_set, img))\n",
        "#       # cv2.imwrite(img[0], input_image)\n",
        "#     print('Complete!')\n",
        "# # main()\n",
        "# dataset = RetinoDataset()\n",
        "# dataset.main()\n",
        "# print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3LxCjrqdfiY"
      },
      "outputs": [],
      "source": [
        "# labels_map = {\n",
        "#     0: \"Mild Diabetic Nonproliferative Retinopathy\",\n",
        "#     1: \"Moderate Diabetic Nonproliferative Retinopathy\",\n",
        "#     2: \"Severe Diabetic Nonproliferative Retinopathy\",\n",
        "#     3: \"Hypertensive Retinopathy\",\n",
        "#     4: \"Proliferative Diabetic Retinopathy\",\n",
        "#     5: \"No Retinopathy\"\n",
        "# }\n",
        "\n",
        "# # figure = plt.figure(figsize=(512, 512))\n",
        "# # cols, rows = 3, 3\n",
        "# # train = '/content/Train_Test_Folder/test/FinalTrainingData'\n",
        "# # test = '/content/Train_Test_Folder/train/FinalTrainingData'\n",
        "# # print(train.count())\n",
        "# # print(test.count())\n",
        "# # training_data = datasets(\n",
        "# #     root=train,\n",
        "# #     train=True,\n",
        "# #     download=True,\n",
        "# #     transform=ToTensor()\n",
        "# # )\n",
        "\n",
        "# # test_data = datasets(\n",
        "# #     root=test,\n",
        "# #     train=False,\n",
        "# #     download=True,\n",
        "# #     transform=ToTensor()\n",
        "# # )\n",
        "# for i in range(1, cols * rows + 1):\n",
        "#     sample_idx = torch.randint(len(train), size=(1,)).item()\n",
        "#     img = train[sample_idx]\n",
        "#     figure.add_subplot(rows, cols, i)\n",
        "#     plt.title(labels_map[0])\n",
        "#     plt.axis(\"off\")\n",
        "#     plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TPEnxn8qUpN"
      },
      "outputs": [],
      "source": [
        "# print(\"Model structure: \", model, \"\\n\\n\")\n",
        "# for name, param in model.named_parameters():\n",
        "  # print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for batch, (X, y) in enumerate(dataloader):\n",
        "#   optimizer.zero_grad()\n",
        "# # Compute prediction and loss\n",
        "#   X = X.cpu()\n",
        "#   y = list(y)\n",
        "#   for i in range(len(y)):\n",
        "#     y[i] = int(y[i])\n",
        "#     if y[i] == 0:\n",
        "#       y[i] = 1\n",
        "#     else:\n",
        "#       y[i] = 0\n",
        "#   y = torch.tensor(y)\n",
        "#   y = y.cpu()\n",
        "#   history = model.fit(X, y, batch_size=8, epochs=50, validation_data=(X_valid, y_valid))\n",
        "# for t in range(epochs):\n",
        "#   print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "#   train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "#   test_loop(test_dataloader, model, loss_fn)\n",
        "# print(\"Done!\")"
      ],
      "metadata": {
        "id": "6NNcrPIwyA6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yK2E7tsJ2_tW"
      },
      "outputs": [],
      "source": [
        "# RYAN COMPUTER CODE\n",
        "# data = '/content/drive/MyDrive/UCSB SRA Project/TrainingData_Final'\n",
        "# print()\n",
        "# dirs = os.listdir(data)\n",
        "# dataset = []\n",
        "# for dir in dirs:\n",
        "#   for inner_dir in os.listdir(os.path.join(data, dir)):\n",
        "#       dataset.append(os.path.join(data, dir, inner_dir))\n",
        "# print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idP8PNy65lXH"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# train_set = []\n",
        "# test_set = []\n",
        "# validation_set = []\n",
        "# for i in range(0, 6):\n",
        "#   path = f\"/content/drive/MyDrive/TrainingData_Final/{i}\"\n",
        "#   all_imgs_0 = os.listdir(path)\n",
        "#   # /content/drive/MyDrive/TrainingData_Final/0\n",
        "#   all_imgs_0 = [f\"{path}/{_path}\" for _path in all_imgs_0]\n",
        "#   num_imgs_0 = len(all_imgs_0)\n",
        "#   arr = np.arange(num_imgs_0)\n",
        "#   np.random.shuffle(arr)\n",
        "#   train_set += [all_imgs_0[idx] for idx in arr[0:int(0.8*num_imgs_0)]]\n",
        "#   test_set += [all_imgs_0[idx]  for idx in arr[int(0.8*num_imgs_0):int(0.9*num_imgs_0)]]\n",
        "#   validation_set += [all_imgs_0[idx]  for idx in arr[int(0.9*num_imgs_0):int(num_imgs_0)]]\n",
        "# print(len(train_set))\n",
        "# print(len(test_set))\n",
        "# print(len(validation_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAZm-HDcyrPg"
      },
      "outputs": [],
      "source": [
        "# for element in range(len(train_set)):\n",
        "#   img = Image.open(train_set[element])\n",
        "#   convert_tensor = transforms.ToTensor()\n",
        "#   train_set[element] = convert_tensor(img)\n",
        "#   print(len(train_set[element]))\n",
        "# for element in range(len(validation_set)):\n",
        "#   img = Image.open(validation_set[element])\n",
        "#   convert_tensor = transforms.ToTensor()\n",
        "#   validation_set[element] = convert_tensor(img)\n",
        "# for element in range(len(test_set)):\n",
        "#   img = Image.open(test_set[element])\n",
        "#   convert_tensor = transforms.ToTensor()\n",
        "#   test_set[element] = convert_tensor(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6akK_ZlCQQuQ"
      },
      "outputs": [],
      "source": [
        "# RYAN COMPUTER CODE\n",
        "# import torch\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from torchvision import transforms\n",
        "# from PIL import Image\n",
        "\n",
        "# labels = []\n",
        "# class CustomDataset(Dataset):\n",
        "#     def __init__(self, file_paths):\n",
        "#         self.file_paths = file_paths\n",
        "#         self.transform = transforms.Compose([\n",
        "#             transforms.Resize((256, 256)),  # Resize the image to (256, 256)\n",
        "#             transforms.ToTensor(),          # Convert the PIL image to a PyTorch tensor\n",
        "#             transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize the tensor\n",
        "#         ])\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.file_paths)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         file_path = self.file_paths[idx]\n",
        "#         image = Image.open(file_path)\n",
        "\n",
        "#         if self.transform:\n",
        "#             image = self.transform(image)\n",
        "\n",
        "\n",
        "#         return image, file_path[59]\n",
        "# train_dataloader = DataLoader(CustomDataset(train_set), batch_size=32, shuffle=True)\n",
        "# test_dataloader = DataLoader(CustomDataset(test_set), batch_size=32, shuffle=True)\n",
        "# val_dataloader = DataLoader(CustomDataset(validation_set), batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "332tWrcXQZkL"
      },
      "outputs": [],
      "source": [
        "# RYAN COMPUTER CODE\n",
        "# train_set = []\n",
        "# test_set = []\n",
        "# validation_set = []\n",
        "# for i in range(0, 6):\n",
        "#   path = f\"/content/drive/MyDrive/UCSB SRA Project/TrainingData_Final/{i}\"\n",
        "#   all_imgs_0 = os.listdir(path)\n",
        "#   # /content/drive/MyDrive/TrainingData_Final/0\n",
        "#   all_imgs_0 = [f\"{path}/{_path}\" for _path in all_imgs_0]\n",
        "#   num_imgs_0 = len(all_imgs_0)\n",
        "#   arr = np.arange(num_imgs_0)\n",
        "#   np.random.shuffle(arr)\n",
        "#   train_set += [all_imgs_0[idx] for idx in arr[0:int(0.05*num_imgs_0)]]\n",
        "#   test_set += [all_imgs_0[idx]  for idx in arr[int(0.05*num_imgs_0):int(0.9*num_imgs_0)]]\n",
        "#   validation_set += [all_imgs_0[idx]  for idx in arr[int(0.9*num_imgs_0):int(num_imgs_0)]]\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "Eedcdmz56TpX"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}